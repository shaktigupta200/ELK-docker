input {
    kafka {
        zk_connect => "127.0.0.1:2181"
        group_id => "adminlog001"
	topic_id => "logs"
        consumer_restart_on_error => true
        consumer_restart_sleep_ms => 100
    }
}



filter {
	json {
		source => "message"
	}
	json {
		source => "description"
		target => "description_json"
	}
	mutate {
		rename => {
			"[description_json][field1]" => "field1"
			"[description_json][field2]" => "field2"
				}
			convert => {
                                       "field1" => "string"
                                       "field2" => "string"
				                              "channel" => "string"
                                       "time_taken" => "float"
                       }
		remove_field => ["description_json"]
	}
	date {
		match => ["log_time","dd/MMM/yyyy:HH:mm:ss Z"]
		target => "@timestamp"
		locale => "en"
		remove_field => [ 'log_time' , 'beat' , 'offset' , 'source' , 'kafka*' , 'input_type' , 'host' , "source" , "host" , "log_time" , "input_type" , "beat"  ]
        }
	#if [module] == "oops" or [module] == "Return_Filing" or [module] == "convertToCod" or [module] == "product_list" or [module] == "traversal" {
  if [module] == "Return_Filing" or [module] == "convertToCod" or [module] == "product_list" or [error_name] == "Postmanifest_sms_params" or [module] == "traversal" {
		drop { }
	}
}

output {
	#stdout { codec => rubydebug }	
	elasticsearch { 
			index => "admin_log-%{+YYYY.MM.dd}"
			hosts => ["127.0.0.1:9200"]
			template => "/etc/logstash/template/admin_log.json"
			template_overwrite => true
			template_name => "admin_log-*"
			workers => 5 
        }
}
